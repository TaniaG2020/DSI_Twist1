# -*- coding: utf-8 -*-
"""HousePricing2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OVdzeLjmtEa6mx-1kZneB-ZsjGAT89lv
"""

# Import the data file
from google.colab import files
uploaded = files.upload()

import io
import pandas as pd

# Read the data
X = pd.read_csv(io.BytesIO(uploaded['houses2.csv']))
X_full = X.copy()

X.corr()

#print a summary of the data
X_full.describe()

"""58 Parking Spaces? Probably very different definitions, not sure if can be used."""

X_full.head()

X_full.columns = ["Price", "Acc_type", "sub", "bed", "bath", "park", "sizeFloor","sizeErf","area"]

X_full.head()

X_full.corr()

"""Removing outliers:
Random Forest: 1565095.537082523
XGBoost : 1442314.659502813
"""

X_testbla = X_full[X_full['Acc_type'] == 'House']

X_test1 = X_testbla[X_testbla['Price'] < 10000000]  
X_test2 = X_test1[X_test1['bed'] < 9]  
X_test3 = X_test2[X_test2['bath'] < 7]  
X_test4 = X_test3[X_test3['park'] < 9]  
#X_test5 = X_test4[X_test4['sizeFloor'] < 220]
X_test5 = X_test4[X_test4['sizeErf'] < 7500]

X_test6 = X_test5[X_test5['Acc_type'] == 'Apartment']

len(X_test5.index)

X_test5.head()

len(X_full.index)

X_test5.describe()

X = X_full.copy()

X_full = X_test5.copy()

X_full.describe()

# Remove rows with missing target, separate target from predictors
X_full.dropna(axis=0, subset=['Price'], inplace=True)
y = X_full.Price              
X_full.drop(['Price'], axis=1, inplace=True)

# Make copy to avoid changing original data (when imputing)
X_test = X_full.copy()

X_test.head()

X_test5.corr()

"""Initially, we just replace Nan with 0 in the floor and erf sizes columns."""

X_test['sizeFloor'] = X_test['sizeFloor'].fillna(0)

X_test['sizeErf'] = X_test['sizeErf'].fillna(0)

X_test.head()

"""Remove "park" because of weird value. Does not improve considerably RMSE: 5607703.4705039365"""

X_test = X_test.drop('park', 1)

X_test.head()

"""Other approach: merge sizes: 5503509.257517554"""

X_test.head()

# Make copy to avoid changing original data (when imputing)
X_test_plus = X_test.copy()

X_test_plus.head()

len(X_test_plus.index)

import numpy as np

for i in X_test_plus.index:
  if pd.notnull(X_test_plus.loc[i,'sizeFloor']):
      X_test_plus.loc[i,'Size_select'] = 1
  elif pd.notnull(X_test_plus.loc[i,'sizeErf']):
      X_test_plus.loc[i,'Size_select'] = 2
  else:
      X_test_plus.loc[i,'Size_select'] = 0    
'''
X_test_plus['Size_select'] = np.where(pd.notnull(),
         1,
         np.where((df.b == "N")&(~df.c.isnull()),
                  df.a*df.c,
                  df.a))
pd.notnull(df.c)
X_test_plus.SizeFloor
'''

X_test_plus

for i in X_test_plus.index:
  if pd.isnull(X_test_plus.loc[i,'Size_select']):
    print(X_test_plus.loc[i,'Size_select'])

X_test_plus.Size_select.astype(bool).sum(axis=0)

len(X_test_plus.index)

X_test_plus.sizeErf.fillna(0, inplace=True)
X_test_plus.sizeFloor.fillna(0, inplace=True)
X_test_plus["Size"] = X_test_plus["sizeFloor"] + X_test_plus["sizeErf"]

X_test_plus.head()

"""Consider removing when size = 0, or changing to "nan""""

X_test_plus = X_test_plus.drop('sizeErf', 1)
X_test_plus = X_test_plus.drop('sizeFloor', 1)

X_test_plus.head()

"""Drop sub column with sizes merged:
Random Forest : 3807190.167069573
XGBoost : 3657114.2022407562
"""

X_test_plus = X_test_plus.drop('sub', 1)

X_test = X_test.drop('sub', 1)

X_test = X_test.drop('sizeFloor', 1)

X_test.head()

X_test_plus.head()

X_test_plus = X_test_plus.drop('sizeErf', 1)

X_test_plus.head()

X_test_plus.Size.replace(0, np.nan, inplace=True)

X_test_plus.head()

"""outliers"""

X_full.park.isnull()

import numpy as np
import scipy.stats as stats
out=[]
def ZRscore_outlier(df):
    med = np.median(df)
    ma = stats.median_absolute_deviation(df)
    for i in df: 
        z = (0.6745*(i-med))/ (np.median(ma))
        if np.abs(z) > 3: 
            out.append(i)
    print("Outliers:",out)
ZRscore_outlier(y)

import numpy as np
import scipy.stats as stats
out=[]
def ZRscore_outlier(df):
    med = np.median(df)
    ma = stats.median_absolute_deviation(df)
    for i in df: 
        z = (0.6745*(i-med))/ (np.median(ma))
        if np.abs(z) > 3: 
            out.append(i)
    print("Outliers:",out)
ZRscore_outlier(y)

out=[]
def Zscore_outlier(df):
    for i in df: 
        z = (i-np.mean(df))/np.std(df)
        if np.abs(z) > 3: 
            out.append(i)
    print("Outliers:",out)
Zscore_outlier(X_testbla['park'])



"""Drop park column, with sizes merged: 5509149.595440613"""

X_test_plus = X_test_plus.drop('park', 1)

X_test_plus.head()

X_test_plus['sub'] = X_full['sub']

X_test_plus.head()

"""Drop suburbs with less than 10 houses: 5110512.005364064"""

v = X_test_plus[['sub']]
X_test_plus1 = X_test_plus[v.replace(v.stack().value_counts()).gt(10).all(1)] 
y_test = y[v.replace(v.stack().value_counts()).gt(10).all(1)]

len(X_test_plus1.index)

len(y_test.index)

X_test_plus = X_test_plus1

len(X_test_plus.index)

X_test_plus = X_test_plus1[X_test_plus['area'] == 'northern-suburbs'].index
X_test_plus = X_test_plus1[X_test_plus['area'] == 'northern-suburbs']

X_test_plus.head()

"""End of data testing, beginning of pipeline and modeling"""

X_test = X_test_plus.copy()

X_test

# Select categorical columns
categorical_cols = [cname for cname in X_test.columns if 
                    X_test[cname].dtype == "object"]

# Select numerical columns
numerical_cols = [cname for cname in X_test.columns if 
                X_test[cname].dtype in ['int64', 'float64']]

# Keep selected columns only
my_cols = categorical_cols + numerical_cols
X_test = X_test[my_cols]

my_cols

X_test.head()

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from xgboost import XGBRegressor 

# Preprocessing for numerical data
numerical_transformer = SimpleImputer(strategy='most_frequent')

# Preprocessing for categorical data
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

"""Random Forest Model"""

# Define model
model_forest = RandomForestRegressor(n_estimators=200, random_state=0)

"""XGBoost Model"""

model_xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05)

"""My pipelines"""

# Bundle preprocessing and modeling code in a pipeline
my_pipeline_forest = Pipeline(steps=[('preprocessor', preprocessor),
                      ('model', model_forest)
                     ])

my_pipeline_xgb = Pipeline(steps=[('preprocessor', preprocessor),
                      ('model', model_xgb)
                     ])

"""Cross_validation"""

from sklearn.model_selection import cross_val_score

# Multiply by -1 since sklearn calculates *negative* MAE
scores = -1 * cross_val_score(my_pipeline_forest, X_test, y,
                              cv=5,
                              scoring='neg_root_mean_squared_error')

print("RMSE scores:\n", scores)
print("Average RMSE score (across experiments):")
print(scores.mean())

sorted(metrics.SCORERS.keys())

from sklearn.model_selection import cross_val_score
# Multiply by -1 since sklearn calculates *negative* RMSE
scores = -1 *  cross_val_score(my_pipeline_xgb, X_test, y,
                              cv=5,
                              scoring='neg_root_mean_squared_error')

print("RMSE scores:\n", scores)
print("Average RMSE score (across experiments):")
print(scores.mean())

params = {
        'model__min_child_weight': [1, 5, 10],
        'model__gamma': [0.5, 1, 1.5, 2, 5],
        'model__subsample': [0.6, 0.8, 1.0],
        'model__colsample_bytree': [0.6, 0.8, 1.0],
        'model__max_depth': [3, 4, 5]
        }

from sklearn.model_selection import RandomizedSearchCV
random_search = RandomizedSearchCV(my_pipeline_xgb, param_distributions=params, n_iter=5, scoring='neg_root_mean_squared_error', n_jobs=4, cv=5, verbose=3, random_state=1001 )

my_pipeline_xgb.get_params().keys()

random_search.fit(X_test, y)

print('\n All results:')
print(random_search.cv_results_)
print('\n Best estimator:')
print(random_search.best_estimator_)
print('\n Best hyperparameters:')
print(random_search.best_params_)

X_full['sub'].nunique()

model_xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05, subsample=1, min_child_weight=5, max_depth=3, gamma=5, colsample_bytree = 1)

my_pipeline_xgb = Pipeline(steps=[('preprocessor', preprocessor),
                      ('model', model_xgb)
                     ])

from sklearn.model_selection import cross_val_score
# Multiply by -1 since sklearn calculates *negative* RMSE
scores = -1 *  cross_val_score(my_pipeline_xgb, X_test, y,
                              cv=5,
                              scoring='neg_root_mean_squared_error')

print("RMSE scores:\n", scores)
print("Average RMSE score (across experiments):")
print(scores.mean())